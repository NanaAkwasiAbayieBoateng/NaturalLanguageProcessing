---
title: "R Twitter"
output: html_notebook
author: Nana Boateng
df_print: paged
Time: '`r Sys.time()`'
date: "`r format(Sys.time(), '%B %d, %Y')`"
---


```{r,warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(rtweet)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lubridate)
library(knitr)
library(wordcloud)
library(tidytext)
library(igraph)
theme_set(theme_light())
```



 

```{r,warning=FALSE,message=FALSE}
#authorize r to read tweets from your timeline
## whatever name you assigned to your created app
appname <- "Nana148"

## api key (example below is not a real key)
key <- "EePxcG88TTYFNEDuUqPKy1B5c"

## api secret (example below is not a real key)
secret <- "G2Cbz79roLFUmccub66zPgNo1uztR7Fo5vIzctJW3TBNTLPPsb"

## create token named "twitter_token"
twitter_token <- create_token(
    app = appname,
    consumer_key = key,
    consumer_secret = secret)
```





```{r,warning=FALSE,message=FALSE}
## path of home directory
home_directory <- path.expand("/Users/nanaakwasiabayieboateng/Documents/memphisclassesbooks/DataMiningscience/Text")

## combine with name for token
file_name <- file.path(home_directory, "twitter_token.rds")

## save token to home directory
saveRDS(twitter_token, file = file_name)

readRDS("/Users/nanaakwasiabayieboateng/Documents/memphisclassesbooks/DataMiningscience/Text/twitter_token", refhook = NULL)

```



Retrieving Tweets


```{r}
## search for 500 tweets using the #rstats hashtag
team_rstats <- search_tweets("#rstats", n = 500)
team_rstats

team_rstats$text

## access and preview data on the users who posted the tweets
users_data(team_rstats) %>%
    head()

## return 200 tweets from @KyloR3n's timeline
kylo_is_a_mole <- get_timeline("KyloR3n", n = 200)
head(kylo_is_a_mole)

## extract emo kylo ren's user data
users_data(kylo_is_a_mole)

## stream tweets mentioning @HillaryClinton for 2 minutes (120 sec)
imwithher <- stream_tweets("HillaryClinton", timeout = 120)
head(imwithher)

## extract data on the users who posted the tweets
head(users_data(imwithher))

## stream 3 random samples of tweets
for (in in seq_len(3)) {
    stream_tweets(q = "", timeout = 60,
        file_name = paste0("rtw", i), parse = FALSE)
    if (i == 3) {
        message("all done!")
        break
    } else {
        # wait between 0 and 300 secs before next stream
        Sys.sleep(runif(1, 0, 300))
    }
}

## parse the samples
tw <- lapply(c("rtw1.json", "rtw2.json", "rtw3.json"),
             parse_stream)

## collapse lists into single data frame
tw.users <- do.call("rbind", users_data(tw))
tw <- do.call("rbind", tw)
attr(tw, "users") <- tw.users

## preview data
head(tw)
users_data(tw) %>%
    head()
```


####Retrieving Users

```{r,warning=FALSE,message=FALSE}
# search for 50 users using "social science" as a keyword
harder_science <- search_users("social science", n = 50)
harder_science

# extract most recent tweets data from the social scientists
tweets_data(harder_science)

## lookup users by screen_name or user_id
users <- c("KimKardashian", "justinbieber", "taylorswift13",
           "espn", "JoelEmbiid", "cstonehoops", "KUHoops",
           "upshotnyt", "fivethirtyeight", "hadleywickham",
           "cnn", "foxnews", "msnbc", "maddow", "seanhannity",
           "potus", "epa", "hillaryclinton", "realdonaldtrump",
           "natesilver538", "ezraklein", "annecoulter")
famous_tweeters <- lookup_users(users)

## preview users data
famous_tweeters

# extract most recent tweets data from the famous tweeters
tweets_data(famous_tweeters)

## or get user IDs of people following stephen colbert
colbert_nation <- get_followers("stephenathome", n = 180)

## get even more by using the next_cursor function
page <- next_cursor(colbert_nation)

## use the page object to continue where you left off
colbert_nation_ii <- get_followers("stephenathome", n = 180, page = page)
colbert_nation <- c(unlist(colbert_nation), unlist(colbert_nation_ii))

## and then lookup data on those users (if hit rate limit, run as two parts)
colbert_nation <- lookup_users(colbert_nation)
colbert_nation

## or get user IDs of people followed *by* President Obama
obama1 <- get_friends("BarackObama")
obama2 <- get_friends("BarackObama", page = next_cursor(obama1))

## and lookup data on Obama's friends
lookup_users(c(unlist(obama1), unlist(obama2)))
```





####Retrieving Trends

```{r}
## get trending hashtags, mentions, and topics worldwide
prestige_worldwide <- get_trends()
prestige_worldwide

## or narrow down to a particular country
usa_usa_usa <- get_trends("United States")
usa_usa_usa

## or narrow down to a popular city
CHIEFS <- get_trends("Kansas City")
CHIEFS
```



####Posting Tweets

```{r}
post_tweet("my first rtweet #rstats")
```



####post_tweet("my first rtweet #rstats")

```{r}
## ty for the follow ;)
post_follow_user("kearneymw")
```






#####get_favorites get_favorites
Description
Returns the 20 most recent Tweets liked by the authenticating or specified user.
Usage
get_favorites(user, n = 3000, since_id = NULL, max_id = NULL,
parse = TRUE, clean_tweets = FALSE, as_double = FALSE, usr = TRUE,
token = NULL)

```{r,message=FALSE,warning=FALSE}

library(twitteR)
library(rtweet)
library(lubridate)
library(tidyverse)
library(tidytext)
library(scales)
library(DT)
library(viridis)
library(stringr)
#authorize r to read tweets from your timeline
## whatever name you assigned to your created app
appname <- "Nana148"

## api key (example below is not a real key)
twitter_api_key <- "EePxcG88TTYFNEDuUqPKy1B5c"

## api secret (example below is not a real key)
secret <- "G2Cbz79roLFUmccub66zPgNo1uztR7Fo5vIzctJW3TBNTLPPsb"

## create token named "twitter_token"
# twitter_api_token <- create_token(
#     app = appname,
#     consumer_key = twitter_api_key,
#     consumer_secret = secret)

#App-only authentication	
twitter_api_token<-"https://api.twitter.com/oauth2/token"

#Request token URL
twitter_api_token<-"https://api.twitter.com/oauth/request_token"

#authorize
twitter_api_token<-"https://api.twitter.com/oauth/authorize"

#Access token URL
twitter_api_token<-"https://api.twitter.com/oauth/access_token"

#Access Token
twitter_api_token<-"237877830-nwGvKBAMyVdoZMAl7zznQiVkaBf8kOM9hadERI2X"

#Access Token Secret
#twitter_api_token<-"w75gmyFQzIQnTISrSUP7mOKvwd7DLcReCgovFYOaJlO8l"

# # You'd need to set global options with an authenticated app
# setup_twitter_oauth(getOption(twitter_api_key),
#                     getOption(secret))
# 
# # We can request only 3200 tweets at a time; it will return fewer
# # depending on the API
# trump_tweets <- userTimeline("realDonaldTrump", n = 3200)
# trump_tweets_df <- trump_tweets %>%
#   map_df(as.data.frame) %>%
#   tbl_df()
```

```{r,message=FALSE,warning=FALSE}
## search for 500 tweets using the realDonaldTrump username
trump_tweets <- search_tweets("realDonaldTrump", n = 3200)

trump_tweets%>%head

#team_rstats$text

## access and preview data on the users who posted the tweets
users_data(trump_tweets) %>%
    head()



```


```{r}
names(trump_tweets)
```


```{r}
#retrieve the text column
#trump_tweets[5]
trump_tweets["text"]
```




```{r}
str(trump_tweets)
```




####Clean up the data
```{r}
# if you want to follow along without setting up Twitter authentication,
# just use this dataset:
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))

```


```{r}

# replacing 'Lorem' with ''
# lorem = "Lorem Ipsum"
# str_sub(lorem, 1, 5) <- ""
# lorem
#df=data_frame(a=c("we","us"),b=c("Twitter for Android","Twitter for iPhone"))
#df1=data_frame(b=c("Twitter for Android","Twitter for iPhone"))
#apply(df1,2,function(x) str_sub(string=x,13,19))




tweets <- trump_tweets %>%
  select(user_id, source, text, created_at)

Device<-data.frame(apply(tweets["source"] ,2,function(x) str_sub(string=x,13,19)))
  
#tweets=tweets%>% mutate(apply(tweets["source"] ,2,function(x) str_sub(string=x,13,19)))  

#tweets=cbind.data.frame(tweets,Device=Device)

tweets=bind_cols(tweets,Device)%>%rename(Device=source1)%>%dplyr::select(-source)%>%

filter(Device %in% c("iPhone", "Android"))

#tweets=tweet%>%select(-c(source)) %>%head()

Device%>%head()
DT::datatable(data.frame(tweets))
```


```{r}
x = "PRODUCT colgate good but not goodOKAY"
library(stringr)
str_extract(string = x, pattern = regex("(?<=PRODUCT).*(?=OKAY)"))


#str_extract(string = x, pattern = perl("(good/not"))
string=c("Twitter for iPhone","Twitter for Android")
pattern="iphone"
str_extract_all(string="Twitter for iPhone", pattern="iphone")
string

str_sub(string="Twitter for Android",13,19)
str_sub(string="Twitter for iPhone",13,19)
```





####Comparison using metadata
One consideration is what time of day the tweets occur, which we’d expect to be a “signature” of their user. Here we can certainly spot a difference:


```{r}
tweets %>%
  count(Device, hour = hour(with_tz(created_at, "EST")),day = day(with_tz(created_at, "EST")))%>%
  mutate(percent = n / sum(n))
```



```{r}

tweets %>%
  mutate( hour = hour(with_tz(created_at, "EST")),day = day(with_tz(created_at, "EST"))) %>%
  group_by(hour)
```


```{r}

#creates an hour variable

# tweets %>%
#   count(Device, hour = hour(with_tz(created_at, "EST"))) %>%
#   mutate(percent = n / sum(n)) %>%
#   ggplot(aes(hour, percent, color = Device)) +
#   geom_line() +
#   scale_y_continuous(labels = percent_format()) +
#   labs(x = "Hour of day (EST)",
#        y = "% of tweets",
#        color = "")


tweets %>%
  mutate( hour = hour(with_tz(created_at, "EST")),day = day(with_tz(created_at, "EST"))) %>%
  group_by(hour)%>%summarise(n())
  mutate(percent = n / sum(n)) %>%
  ggplot(aes(hour, percent, color = Device)) +
  geom_line() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```

Trump on the Android does a lot more tweeting in the morning, while the campaign posts from the iPhone more in the afternoon and early evening.

Another place we can spot a difference is in Trump’s anachronistic behavior of “manually retweeting” people by copy-pasting their tweets, then surrounding them with quotation marks:

```{r}
tweets %>%
  count(Device,
        quoted = ifelse(str_detect(text, '^"'), "Quoted", "Not quoted")) %>%
  ggplot(aes(Device, n, fill = quoted)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "", y = "Number of tweets", fill = "") +
  ggtitle('Whether tweets start with a quotation mark (")')
```



Comparison of words
Now that we’re sure there’s a difference between these two accounts, what can we say about the difference in the content? We can use the tidytext package to analyze this.

We start by dividing into individual words using the unnest_tokens function, and removing some common “stopwords”. This is a common aspect to preparing text for analysis. Typically, tokens are single words from a document. However they can also be bi-grams (pairs of words), tri-grams (three-word sequences), n-grams (n
-length sequences of words), or in this case, individual words, hashtags, or references to other Twitter users. Because tweets are a special form of text (they can include words, urls, references to other users, hashtags, etc.) we need to use a custom tokenizing function to convert the text into tokens.


```{r}
library(tidytext)

reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"   # custom regular expression to tokenize tweets

# function to neatly print the first 10 rows using kable
print_neat <- function(df){
  df %>%
    head() %>%
    knitr::kable()
}

# tweets data frame
tweets %>%
  print_neat()
```



```{r}
# remove manual retweets
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  print_neat()
```




```{r}
# remove urls
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  print_neat()
```



```{r}
# unnest into tokens - tidytext format
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  print_neat()
```




```{r}
# remove stop words
tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>%
  print_neat()
```




```{r}
# store for future use
tweet_words <- tweets %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
```



####Assessing word and document frequency
One measure of how important a word may be is its term frequency (tf), how frequently a word occurs within a document. The problem with this approach is that some words occur many times in a document, yet are probably not important (e.g. “the”, “is”, “of”). Instead, we want a way of downweighting words that are common across all documents, and upweighting words that are frequent within a small set of documents.

Another approach is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. This can be combined with term frequency to calculate a term’s tf-idf, the frequency of a term adjusted for how rarely it is used. It is intended to measure how important a word is to a document in a collection (or corpus) of documents. It is a rule-of-thumb or heuristic quantity, not a theoretically proven method. The inverse document frequency for any given term is defined as



$idf(term)=\log\frac{n_{documents}}{n_{documents containing term}}$


To calculate tf-idf for this set of documents, we will pool all the tweets from iPhone and Android together and treat them as if they are two total documents. Then we can calculate the frequency of terms in each group, and standardize that relative to the the term’s frequency across the entire corpus.


```{r}
tweet_words_count <- tweet_words %>%
  count(source, word, sort = TRUE) %>%
  ungroup()
tweet_words_count

```




```{r}
total_words <- tweet_words_count %>%
  group_by(source) %>%
  summarize(total = sum(n))
total_words
```




```{r}
tweet_words_count <- left_join(tweet_words_count, total_words)
tweet_words_count
```



```{r}
tweet_words_count <- tweet_words_count %>%
  bind_tf_idf(word, source, n)
tweet_words_count
```

Which terms have a high tf-idf?
```{r}
tweet_words_count %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```



```{r}
tweet_important <- tweet_words_count %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

tweet_important %>%
  group_by(source) %>%
  slice(1:15) %>%
  ggplot(aes(word, tf_idf, fill = source)) +
  geom_bar(alpha = 0.8, stat = "identity") +
  labs(title = "Highest tf-idf words in @realDonaldTrump",
       subtitle = "Top 15 for Android and iPhone",
       x = NULL, y = "tf-idf") +
  coord_flip()
```



*Most hashtags come from the iPhone. Indeed, almost no tweets from Trump’s Android contained hashtags, with some rare exceptions like this one. (This is true only because we filtered out the quoted “retweets”, as Trump does sometimes quote tweets like this that contain hashtags).

*Words like “join”, and times like “7pm”, also came only from the iPhone. The iPhone is clearly responsible for event announcements like this one (“Join me in Houston, Texas tomorrow night at 7pm!”)

*A lot of “emotionally charged” words, like “badly” and “dumb”, were overwhelmingly more common on Android. This supports the original hypothesis that this is the “angrier” or more hyperbolic account.

Sentiment analysis
Since we’ve observed a difference in sentiment between the Android and iPhone tweets, let’s try quantifying it. We’ll work with the NRC Word-Emotion Association lexicon, available from the tidytext package, which associates words with 10 sentiments: positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.


```{r}
nrc <- sentiments %>%
  filter(lexicon == "nrc") %>%
  select(word, sentiment)
nrc
```

To measure the sentiment of the Android and iPhone tweets, we can count the number of words in each category:

```{r}
sources <- tweet_words %>%
  group_by(source) %>%
  mutate(total_words = n()) %>%
  ungroup() %>%
  distinct(id, source, total_words)
sources
```



```{r}
by_source_sentiment <- tweet_words %>%
  inner_join(nrc, by = "word") %>%
  count(sentiment, id) %>%
  ungroup() %>%
  complete(sentiment, id, fill = list(n = 0)) %>%
  inner_join(sources) %>%
  group_by(source, sentiment, total_words) %>%
  summarize(words = sum(n)) %>%
  ungroup()

head(by_source_sentiment)
```


(For example, we see that 321 of the 4901 words in the Android tweets were associated with “anger”). We then want to measure how much more likely the Android account is to use an emotionally-charged term relative to the iPhone account. Since this is count data, we can use a Poisson test to measure the difference:

```{r}
# function to calculate the poisson.test for a given sentiment
poisson_test <- function(df){
  poisson.test(df$words, df$total_words)
}

# use the nest() and map() functions to apply poisson_test to each sentiment and 
# extract results using broom::tidy()
sentiment_differences <- by_source_sentiment %>%
  group_by(sentiment) %>%
  nest() %>%
  mutate(poisson = map(data, poisson_test),
         poisson_tidy = map(poisson, tidy)) %>%
  unnest(poisson_tidy, .drop = TRUE)
sentiment_differences
```


And we can visualize it with a 95% confidence interval:


```{r}
sentiment_differences %>%
  ungroup() %>%
  mutate(sentiment = reorder(sentiment, estimate)) %>%
  mutate_each(funs(. - 1), estimate, conf.low, conf.high) %>%
  ggplot(aes(estimate, sentiment)) +
  geom_point() +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  scale_x_continuous(labels = percent_format()) +
  labs(x = "% increase in Android relative to iPhone",
       y = "Sentiment")
```



Thus, Trump’s Android account uses about 40-80% more words related to disgust, sadness, fear, anger, and other “negative” sentiments than the iPhone account does. (The positive emotions weren’t different to a statistically significant extent).

We’re especially interested in which words drove this different in sentiment. Let’s consider the words with the largest changes within each category:


```{r}
tweet_important %>%
  inner_join(nrc, by = "word") %>%
  filter(!sentiment %in% c("positive", "negative")) %>%
  mutate(sentiment = reorder(sentiment, -tf_idf),
         word = reorder(word, -tf_idf)) %>%
  group_by(sentiment) %>%
  top_n(10, tf_idf) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = source)) +
  facet_wrap(~ sentiment, scales = "free", nrow = 4) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "",
       y = "tf-idf") +
  scale_fill_manual(name = "", labels = c("Android", "iPhone"),
                    values = c("red", "lightblue"))
```



This confirms that lots of words annotated as negative sentiments are more common in Trump’s Android tweets than the campaign’s iPhone tweets. It’s no wonder Trump’s staff took away his tweeting privileges for the remainder of the campaign.


```{r}
devtools::session_info()
```

